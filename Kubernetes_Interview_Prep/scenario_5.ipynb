{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180d7b5a",
   "metadata": {},
   "source": [
    "## Mock Scenario Question 5: Pods Can’t Start During Rollout (Image Pull + Probes + Multi-Tenant Constraints)\n",
    "\n",
    "### The Scenario\n",
    "\n",
    "You run a self-managed Kubernetes cluster with 2 namespaces: `prod` and `batch` (shared cluster, strict isolation rules).\n",
    "\n",
    "A rollout of `orders-api` in `prod` starts failing during peak hours:\n",
    "\n",
    "* New pods for `orders-api` are stuck in **`ImagePullBackOff`** for several minutes, then some recover, some don’t.\n",
    "* A subset of pods that do start enter **`CrashLoopBackOff`** within 1–2 minutes.\n",
    "* The old ReplicaSet pods are still serving, but latency is increasing and you’re seeing **sporadic 503s** from the Ingress.\n",
    "* `kubectl get events -n prod` shows a mix of:\n",
    "\n",
    "  * `Failed to pull image ... dial tcp: i/o timeout`\n",
    "  * `Back-off pulling image`\n",
    "  * `Readiness probe failed: HTTP probe failed with statuscode: 503`\n",
    "* Nodes show normal CPU/memory, but **egress bandwidth is high** during peak hours.\n",
    "* You also know `batch` namespace runs heavy nightly jobs that pull large images and hit external APIs.\n",
    "\n",
    "### The Question\n",
    "\n",
    "Give your **step-by-step troubleshooting plan** to restore stability and complete the rollout safely. Cover:\n",
    "\n",
    "1. How you determine whether the primary issue is **registry/network**, **node-level image cache/container runtime**, **application crash**, or **probe misconfiguration**.\n",
    "2. The **highest-signal checks** you’ll run (commands + what you’re looking for).\n",
    "3. **Safe mitigations** you’d apply immediately to reduce user impact (without breaking multi-tenancy isolation).\n",
    "4. Longer-term fixes to prevent recurrence (think: image strategy, registry mirroring/caching, QoS, quotas, scheduling isolation, probe tuning).\n",
    "\n",
    "---\n",
    "\n",
    "Ask your clarification questions first (I’ll answer short, like an interviewer). Then give your final response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199879c8",
   "metadata": {},
   "source": [
    "* **Is it new?** No — `orders-api` has been running in `prod` for **~9 months**.\n",
    "\n",
    "* **When did the failures start?** This began **today at ~11:30 AM ET**, right when traffic peaked.\n",
    "\n",
    "* **When did this rollout start?** The new rollout was triggered at **11:20 AM ET** (CI pipeline pushed a new image tag).\n",
    "\n",
    "* **What changed in this rollout?** Only the **container image tag** (same Deployment, same manifests). No intentional probe changes.\n",
    "\n",
    "* **Extra context:** The image is hosted on an **external registry** (not inside the cluster).\n",
    "---\n",
    "\n",
    "MY FINAL RESPONSE:\n",
    "With the new information, it's great. Now, we also have the data that the kubectl get events from the namespace prod of the new rollout pods is giving us failed to pull image and backoff pulling image and readiness probe failed alerts. So the first two events related to images are quite suspicious.And I would also check the logs of these pods using kubectl logs, hyphen n namespace and pod name. So that may give us a bit more information on the image pulling and the readiness probe. But considering the image pulling issues and the readiness probe issues, and also there is some information that the egress bandwidth is high during this peak hour. So egress meaning the traffic that's going out, the bandwidth to pull the image, I assume. So let's check the details that we are pulling from the image or the image itself. In the latest data, it is mentioned that only the container image tag has been changed. So I would first and foremost check if the name tag is correct. And second, since this is an external registry, I would check if there is proper authentication and authorization to the external registry to pull the image. So by doing these two checks, we'll get a good idea where the problem is. If the tags are correct and if we have the right access to pull the image from the external registry, things should be smooth.Plus, it is also mentioned that from the batch namespace, it runs heavy nightly jobs that pulls larger images and hit external APIs. So the egress could be again heavy, but what we can do here is both should not happen on the same time. The action in the prod namespace, as we can see usually in the morning, and the batch jobs should be scheduled and should be run in the night times so that the load is equally distributed.And finally, I would also take a quick look in the probe, the readiness probe. I would check why is this failing? Is the timeout too less or in order to download larger images, it may need more buffer time. So I would also try tweaking the readiness probe. So my immediate actions would be the name tag verifications and access to the registry like are we authenticated and authorized to pull images and as well as to increase the limits for in the readiness probe in order to facilitate pulling larger images. And for long-term fixes, we should consider the image strategy and as well as executing the bad jobs in the night away from the peak time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7961a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ace5849e",
   "metadata": {},
   "source": [
    "✅ Confirmed — that’s your final response.\n",
    "\n",
    "## Score: **5.5 / 10**\n",
    "\n",
    "### What you did well\n",
    "\n",
    "* You focused on the most visible signals: **ImagePullBackOff + i/o timeout + high egress**.\n",
    "* You recognized multi-tenant impact: `batch` jobs can compete for egress and external APIs.\n",
    "\n",
    "### Where it missed (why it’s not yet “same difficulty” quality)\n",
    "\n",
    "You drifted into assumptions that don’t fit the evidence and missed the highest-signal checks + safest mitigations.\n",
    "\n",
    "Key gaps:\n",
    "\n",
    "1. **Readiness probe has nothing to do with image pulling**\n",
    "\n",
    "* Image pull happens **before** the container starts. A readiness probe can’t “facilitate pulling larger images.”\n",
    "* The readiness 503 is about the app not being ready (or dependency failing), **after** the container starts.\n",
    "\n",
    "2. **You didn’t split the problem into the two distinct failure modes**\n",
    "   This scenario has two separate classes:\n",
    "\n",
    "* **A) ImagePullBackOff**: registry/network/egress/DNS/proxy/auth/cache/container runtime\n",
    "* **B) CrashLoopBackOff / readiness 503**: app startup config, downstream dependency, DB, secrets, env vars, migrations, probes too strict\n",
    "\n",
    "3. **Your checks weren’t specific enough**\n",
    "   Most important checks you didn’t mention:\n",
    "\n",
    "* `kubectl describe pod <pod>` → exact pull error (TLS, timeout, 401, DNS)\n",
    "* Node-level runtime logs: containerd/docker pull errors\n",
    "* From node: `crictl pull <image>` (or equivalent) to isolate kubelet vs registry\n",
    "* `kubectl get events --sort-by=.lastTimestamp`\n",
    "* Compare: `curl` inside pod to its dependencies (DB/cache), because readiness is failing with 503\n",
    "\n",
    "4. **Mitigation should prioritize stability**\n",
    "   You didn’t mention the obvious “stop the bleeding” actions:\n",
    "\n",
    "* **Pause/rollback the rollout** (safe because old RS is healthy)\n",
    "* Temporarily set `maxUnavailable: 0` to preserve serving capacity during peak\n",
    "* Temporarily reduce `batch` egress impact (quota, limit, separate nodes, or schedule)\n",
    "\n",
    "Scheduling batch “only at night” is a nice longer-term goal, but it’s not a reliable immediate fix (and doesn’t explain why it’s happening right now unless batch is actually running now—which the scenario didn’t confirm).\n",
    "\n",
    "---\n",
    "\n",
    "## What a 9/10 answer looks like (brief template)\n",
    "\n",
    "### 1) Stabilize\n",
    "\n",
    "* `kubectl rollout pause deploy/orders-api -n prod` (or rollback if needed)\n",
    "* Ensure old RS stays serving (`maxUnavailable: 0` temporarily)\n",
    "\n",
    "### 2) Triage ImagePullBackOff (fast proof)\n",
    "\n",
    "* `kubectl describe pod -n prod <newpod>` → read Events\n",
    "* Check if error is:\n",
    "\n",
    "  * `i/o timeout` (network/egress/DNS)\n",
    "  * `401/403` (auth)\n",
    "  * `x509` (TLS/CA)\n",
    "  * `manifest unknown` (bad tag)\n",
    "* On an affected node:\n",
    "\n",
    "  * `crictl pull <image>` (bypass k8s objects) to confirm node→registry connectivity\n",
    "* Check if only some nodes fail → suggests node-level egress path, DNS, NAT, or runtime cache issues\n",
    "\n",
    "### 3) Triage CrashLoop / readiness 503 (separately)\n",
    "\n",
    "* `kubectl logs -p` and `kubectl describe` for restart reason\n",
    "* Exec into a running pod and hit:\n",
    "\n",
    "  * `curl localhost:<port>/health`\n",
    "  * `curl <dependency>` (DB/redis/external API)\n",
    "* Confirm probe config:\n",
    "\n",
    "  * readiness endpoint correct? needs initialDelaySeconds? startupProbe missing?\n",
    "\n",
    "### 4) Mitigations\n",
    "\n",
    "* Scale CoreDNS/egress path? (if DNS/reg connectivity is slow)\n",
    "* Add image pull caching/mirroring; pre-pull on nodes\n",
    "* Enforce tenant isolation:\n",
    "\n",
    "  * `ResourceQuota` / limit egress-heavy batch\n",
    "  * separate node pool with taints for batch\n",
    "  * registry mirror inside network\n",
    "\n",
    "---\n",
    "\n",
    "## Topics to skim in your books (targeted)\n",
    "\n",
    "**Kubernetes in Action**\n",
    "\n",
    "* Pod lifecycle + kubelet image pulling and Events\n",
    "* Probes: readiness vs liveness vs startupProbe\n",
    "\n",
    "**Kubernetes Up & Running**\n",
    "\n",
    "* Deployments rollout mechanics (pause/rollback, maxUnavailable/maxSurge)\n",
    "* Debugging patterns with `describe`, events, logs\n",
    "\n",
    "**Kubernetes Patterns**\n",
    "\n",
    "* Deployment patterns (rollout safety, progressive delivery basics)\n",
    "* Multi-tenant isolation patterns (taints, quotas, separate node pools)\n",
    "* Image management patterns (registry mirror/cache, pre-pull)\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can ask a follow-up as the interviewer: **“What exact 3 commands do you run first, and what outcomes would make you rollback vs keep investigating?”**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
