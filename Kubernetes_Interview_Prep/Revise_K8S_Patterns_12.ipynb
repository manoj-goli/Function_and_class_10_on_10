{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51599a11",
   "metadata": {},
   "source": [
    "Here’s a **highest-ROI, interview-ready cheat sheet** pulled from *Kubernetes Patterns* (PDF: ). I’m listing **the “Problem → Solution → Interview soundbite / gotcha”** items most likely to help in a Google Kubernetes TSE conversation.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Health Probe (Liveness vs Readiness) — **automation + troubleshooting gold**\n",
    "\n",
    "**Problem:** “Process is running” is not the same as “app is healthy.” Apps can hang, deadlock, OOM inside the runtime, etc. \n",
    "**Solution:**\n",
    "\n",
    "* **Liveness** detects “must restart” failures (Kubelet restarts container). \n",
    "* **Readiness** detects “don’t send traffic yet/for now” (remove from Service endpoints; no restart). \n",
    "* Readiness gates rolling updates: only when readiness passes is a Deployment considered successful for progressing the rollout. \n",
    "  **Interview soundbite:** “Readiness prevents bad rollouts and protects users; liveness self-heals stuck processes.”\n",
    "\n",
    "  The significance of a readiness probe in Kubernetes rolling deployments is to ensure zero-downtime updates by preventing traffic from being routed to new Pods until they are fully functional and ready to handle requests.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Predictable Demands (requests/limits, QoS) — **scheduling + outages**\n",
    "\n",
    "**Problem:** If you don’t size containers, the scheduler can’t place workloads well; and runtime behavior under pressure becomes unpredictable. \n",
    "**Solution:**\n",
    "\n",
    "* Understand **compressible vs incompressible** resources: CPU throttles, memory kills. \n",
    "* Set **requests** (minimum) and **limits** (max). Scheduler uses **requests (not limits)** for placement. \n",
    "* QoS classes matter during node pressure: **Best-Effort / Burstable / Guaranteed**. \n",
    "  **Interview soundbite:** “Most ‘random evictions/OOMs’ start with bad requests/limits.”\n",
    "\n",
    "  Key Best Practices for QoS:\n",
    "Guaranteed QoS: Set CPU and memory requests equal to limits for critical services to ensure guaranteed resources.\n",
    "Burstable QoS: Set limits higher than requests to allow apps to handle spikes, but keep the ratio reasonable (e.g., \n",
    " the request).\n",
    "Resource Monitoring: Use tools like Prometheus, Grafana, or kubectl top to monitor actual usage vs. allocated, adjusting accordingly.\n",
    "Avoid BestEffort: Never use BestEffort in production, as these pods are the first to be killed under resource contention.\n",
    "Memory vs. CPU: Memory is incompressible (leads to OOMKilled), making strict limits essential. CPU is compressible (throttling), allowing for more flexible, burstable configurations.\n",
    "Namespace Quotas: Define resource quotas per namespace to manage aggregate resource usage. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Declarative Deployment (Rolling, Canary, Blue-Green) — **zero-downtime thinking**\n",
    "\n",
    "**Problem:** Imperative rollouts are brittle (client-side orchestration, hard to repeat, drifts over time). \n",
    "**Solution:**\n",
    "\n",
    "* Use **Deployment** + **RollingUpdate**, shape rollout with **maxSurge/maxUnavailable**, and **don’t forget readiness**. \n",
    "* Canary: run a small ReplicaSet of the new version, direct some traffic, then scale new up/old down. \n",
    "  **Interview soundbite:** “Readiness is what turns ‘rolling update’ into ‘safe rolling update’.”\n",
    "\n",
    "```\n",
    "Primary Deployment: (you existing stable deployment)\n",
    "\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: myapp-primary\n",
    "spec:\n",
    "  replicas: 5\n",
    "  selector: \n",
    "    matchLabels: \n",
    "      app: myapp \n",
    "      version: primary \n",
    "  template: \n",
    "    metadata: \n",
    "      labels: \n",
    "        app: myapp\n",
    "        version: primary \n",
    "    spec: \n",
    "      containers: \n",
    "      - name: myapp \n",
    "        image: myapp:1.0\n",
    "\n",
    "Canary deployment (new version, we are rolling with only 1 replica):\n",
    "\n",
    "apiVersion: apps/v1 \n",
    "kind: Deployment \n",
    "metadata: \n",
    "  name: myapp-canary \n",
    "spec: \n",
    "  replicas: 1\n",
    "  selector: matchLabels: \n",
    "    app: myapp \n",
    "    version: canary \n",
    "  template: \n",
    "    metadata: \n",
    "      labels: \n",
    "        app: myapp\n",
    "        version: canary \n",
    "    spec: \n",
    "      containers: \n",
    "      - name: myapp \n",
    "        image: myapp:1.1\n",
    "  ```\n",
    "\n",
    "\n",
    "kubectl create deployment web-stable --image=nginx:1.26 --replicas=9\n",
    "kubectl scale deployment web-stable --replicas=9\n",
    "\n",
    "kubectl expose deployment web-stable --port=80 --target-port=80 --name=web-service\n",
    "\n",
    "\n",
    "kubectl create deployment web-canary --image=nginx:1.27 --replicas=1\n",
    "\n",
    "# Scale canary up to take more traffic\n",
    "kubectl scale deployment web-canary --replicas=2\n",
    "# Scale stable down to reduce traffic\n",
    "kubectl scale deployment web-stable --replicas=8\n",
    "\n",
    "However, the above imperative commands are only to give you idea, it is not prod best practice, it is not repeatable and can easily drift\n",
    "Canary goes better along with ingress, there you can route traffic, regions\n",
    "Main story: Readiness probe and Rollouts are best friends, dont make the miss each other, you will miss your prod\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Elastic Scale — HPA/VPA/CA + the **HPA+VPA conflict insight**\n",
    "\n",
    "### 4A) HPA: picking the right metric\n",
    "\n",
    "**Problem:** Autoscaling is easy to misconfigure; some metrics don’t correlate with replica count. \n",
    "**Solution:** Pick metrics that drop when you add replicas (CPU or QPS often works; memory often doesn’t unless the app truly redistributes memory). \n",
    "\n",
    "To deal with memory spikes in Kubernetes, the Horizontal Pod Autoscaler (HPA) should not be the primary tool because memory usage often does not scale linearly with the number of pods. Memory spikes are better addressed through a combination of proper resource allocation (Vertical Pod Autoscaler) and proactive strategies (like setting appropriate resource limits and requests) to manage the underlying cause of the spikes\n",
    "\n",
    "### 4B) HPA: the formula + “CPU% is based on requests”\n",
    "\n",
    "HPA computes desired replicas from current vs desired metric value. \n",
    "For resource metrics, the percentage is based on **container requests, not limits**. \n",
    "\n",
    "$$DesiredReplicas = \\lceil CurrentReplicas \\times \\frac{CurrentMetricValue}{TargetMetricValue} \\rceil$$\n",
    "\n",
    "In k8s, (not in general linux), cpu% is based on the requests meaning, the configured resource request in container\n",
    "\n",
    "### 4C) Delays and thrash control (why it reacts “late”)\n",
    "\n",
    "Scaling is a pipeline (cAdvisor → kubelet → metrics server → HPA loop), and deliberate smoothing adds delay; tuning is tradeoffs. \n",
    "\n",
    "3 Important \"Rules\" the HPA FollowsTo prevent your cluster from constantly \"jittering\" (scaling up and down every few seconds), Kubernetes adds some safety logic:\n",
    "\n",
    "The 10% Tolerance: By default, the HPA will not scale if the ratio ($Current / Target$) is between 0.9 and 1.1. This prevents tiny fluctuations from triggering a scale event.\n",
    "\n",
    "Missing Metrics: If one pod in the group isn't reporting CPU (e.g., it's still starting up), the HPA ignores it during the calculation to avoid \"skewing\" the average.\n",
    "\n",
    "Cooldown/Stabilization: HPA usually waits (default is 5 minutes for scale-down) to ensure the load is actually gone before killing pods.\n",
    "\n",
    "### 4D) VPA: disruption + update modes\n",
    "\n",
    "VPA can recommend/apply request changes; **Auto** can evict pods to apply changes (disruptive). \n",
    "VPA recommender->keeps watching metrics server // VPA updater for evictions // VPA admission controller for intercepting evicts and create new bumped pod. VPA recommender gets in touch with both\n",
    "\n",
    "MODES: off // initial // recreate // auto*\n",
    "\n",
    "### 4E) **Why HPA + VPA together (same resource metrics) is a problem**\n",
    "\n",
    "**Core issue:** they’re not aware of each other, so you can get “double scaling” and weird feedback loops. \n",
    "**Interview-ready explanation (tie it to the math):**\n",
    "\n",
    "* If HPA uses CPU% and CPU% is computed vs **requests** \n",
    "* …and VPA changes requests, then the **same real CPU usage suddenly looks like a different percentage**, so HPA may scale up/down unexpectedly.\n",
    "  **Practical guidance (what to say you’d do):**\n",
    "* Use VPA **Off/Initial** for right-sizing signals, then set stable requests; run HPA on CPU/QPS/custom metrics after. \n",
    "\n",
    "| Scaler | Metric | Mode | Best Use Case |\n",
    "| --- | --- | --- | --- |\n",
    "| **HPA** | CPU | Active | Handling sudden traffic spikes. |\n",
    "| **VPA** | CPU/Mem | **Off** | Long-term \"Right-sizing\" advice. |\n",
    "| **HPA** | QPS | Active | Scaling before the CPU even gets hot. |\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Managed Lifecycle (SIGTERM, grace period, hooks) — **reliability during restarts**\n",
    "\n",
    "**Problem:** Pods get killed/restarted for many reasons; if the app doesn’t shut down cleanly you drop in-flight work. \n",
    "**Solution:**\n",
    "\n",
    "* Handle **SIGTERM** quickly and gracefully; Kubernetes waits a default grace period (~30s) before SIGKILL. \n",
    "* Hooks exist (postStart/preStop), but don’t put critical logic there without caution. \n",
    "  **Interview soundbite:** “Graceful shutdown is part of SLOs: readiness + SIGTERM handling go together.”\n",
    "\n",
    "What is SIGTERM?\n",
    "SIGTERM (Signal Terminate) is a signal sent to a process to request its termination. In Kubernetes:\n",
    "When Kubernetes decides to stop a pod, it sends a SIGTERM signal to the main process (PID 1) in each container\n",
    "It's a polite request to shut down, not a forceful kill\n",
    "The process can catch this signal and perform cleanup operations\n",
    "\n",
    "Best Practices\n",
    "Set appropriate grace periods:\n",
    "Too short → processes get killed abruptly\n",
    "Too long → deployment updates are slow\n",
    "\n",
    "Common patterns:\n",
    "\n",
    "Rolling updates: New pods start, old pods get preStop hook to drain connections\n",
    "Stateful applications: Save data during preStop, restore during postStart\n",
    "Service mesh: Properly deregister from service mesh before termination\n",
    "\n",
    "Debugging Tips\n",
    "Check pod events for hook failures:\n",
    "kubectl describe pod <pod-name>\n",
    "kubectl logs <pod-name> --previous  # Check logs of terminated container\n",
    "---\n",
    "\n",
    "## 6) Service Discovery (ClusterIP, Headless, NodePort, LB, Ingress) — **network path clarity**\n",
    "\n",
    "**Problem:** Pods are ephemeral; you need stable discovery for consumers.\n",
    "**Solution:** Use the Service abstraction; common mechanisms summarized: ClusterIP, Headless, NodePort, LoadBalancer, Ingress, plus manual options. \n",
    "**Interview soundbite:** “When debugging ‘service broken’, I think: Service selector → Endpoints → Pod readiness.”\n",
    "\n",
    "Headless = When a client sends a request to a headless Service, it will get back a list of all the Pods that this Service represents. (Good for statefulsets is of the uses)\n",
    "\n",
    "Also endpoints, kube proxy, iptables along with its commands. Endpoints are directly works with readiness probes\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Automated Placement (affinity, taints/tolerations) — **multi-tenant realism**\n",
    "\n",
    "**Problem:** Default scheduling isn’t enough when you need topology, isolation, or hardware constraints.\n",
    "**Solution:** Progress from simple to complex: nodeSelector → affinities → taints/tolerations → custom scheduler; keep intervention minimal.  \n",
    "**Interview soundbite:** “Prefer labels + policies over pinning pods to nodes (nodeName is last resort).” \n",
    "\n",
    "nodeSelector - not recommended unless while troubleshooting something\n",
    "node affinity - more expressive with In, Notin, Exists, DoesNotExit, Gt, Lt\n",
    "Pod Affinity and Pod Anti-Affinity: Topology aware, if you want them to co-exist or spread apart\n",
    "Taints and Tolerations - Only pods with exact tolerations can be scheduled to the tainted node\n",
    "\n",
    "Taint effects:\n",
    "NoSchedule: Don't schedule new pods\n",
    "PreferNoSchedule: Try to avoid scheduling\n",
    "NoExecute: Evict existing pods without toleration\n",
    "\n",
    "custom scheduler: nginx\n",
    "---\n",
    "\n",
    "## 8) Pod Disruption Budget — **non-disruptive maintenance**\n",
    "\n",
    "**Problem:** Drains/cluster scale-down can evict too many replicas at once and break quorum or capacity.\n",
    "**Solution:** PDB limits voluntary evictions (e.g., drain/maintenance), ensuring minAvailable (or maxUnavailable). \n",
    "**Interview soundbite:** “PDB is how you make node maintenance ‘boring’ for critical services.” \n",
    "\n",
    "Manual: \n",
    "kubectl cordon <nodename> // kubectl drain <nodename> // kubectl uncordon <nodename>\n",
    "\n",
    "$kubectl create pod-disruptionbudget --> minAvailable, maxAvailable, labels\n",
    "\n",
    "ETCD Quorum = (n + 1) // 2  (integer division)\n",
    "Or more precisely: floor(n/2) + 1\n",
    "3-node cluster: Needs 2 of 3 to agree\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "## 9) Configuration Resource (ConfigMap/Secret) — **debugging config issues**\n",
    "\n",
    "**Problem:** Env vars don’t scale well for complex config and become hard to track/override. \n",
    "**Solution:**\n",
    "\n",
    "* Use **ConfigMap/Secret** as key-value stores; consume as **env vars** or **mounted files**. \n",
    "* Mounted-file configs **can update on ConfigMap change**, but env-var configs **won’t update without restart**. \n",
    "  **Interview soundbite:** “If config changed but pods didn’t, I ask: env var or mounted file?” \n",
    "\n",
    "  Config not updating in pods?\n",
    "│\n",
    "├─→ Is it consumed as ENV VAR?\n",
    "│   └─→ Must restart pods (rollout restart)\n",
    "│\n",
    "├─→ Is it consumed as MOUNTED FILE?\n",
    "│   ├─→ Does app watch for file changes?\n",
    "│   │   ├─→ Yes → Should auto-update\n",
    "│   │   └─→ No → Still needs restart\n",
    "│   └─→ Check: mount path correct? subPath used?\n",
    "│\n",
    "└─→ Is Secret updated? (Secrets update slower - kubelet sync period) -> Kubelet syncs secrets every 60-360 seconds\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "## 10) Self Awareness (Downward API) — **production-grade “introspection”**\n",
    "\n",
    "**Problem:** Apps sometimes need runtime facts (pod IP/name, namespace, resource limits) without hardcoding or calling the API. \n",
    "**Solution:** Downward API injects metadata via env vars/files; app stays Kubernetes-agnostic. \n",
    "\n",
    "\"The Downward API gives pods their own metadata without coupling apps to Kubernetes APIs. It's like handing someone their ID card—they know who they are without having to ask the DMV. Your app just reads files or env vars, staying Kubernetes-agnostic while being cloud-native.\"\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "## 11) Configuration Template / Config size limits — **one-liner that impresses**\n",
    "\n",
    "If you push giant configs into ConfigMaps/Secrets, you hit size/complexity limits; the book notes an **etcd-backed 1 MB limit** on total values and warns about embedding complex files. \n",
    "**Interview soundbite:** “For large configs, template at startup or use another pattern—don’t fight ConfigMap limits.”\n",
    "\n",
    "etcd is NOT a file system!\n",
    "├─→ Designed for consistent, fast key-value storage\n",
    "├─→ Not for blob storage or large files\n",
    "├─→ 1MB limit prevents performance degradation\n",
    "└─→ Protects the entire cluster from one bad config\n",
    "\n",
    "Need to store config?\n",
    "│\n",
    "├─→ < 1MB, static? → ConfigMap is perfect\n",
    "│\n",
    "├─→ > 1MB but can split? → Multiple ConfigMaps\n",
    "│\n",
    "├─→ > 1MB, dynamic, templates? → Template at startup\n",
    "│\n",
    "├─→ > 1MB, rarely changes? → Init container + object storage\n",
    "│\n",
    "└─→ > 1MB, changes often? → Dedicated config service\n",
    "\n",
    "ConfigMaps are for configuration, not file storage. If you need more than 1MB, you're probably doing something that belongs in a different system. Template, split, or serve it from elsewhere. Use HELM for templating + monitor in CI/CD etc\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "## 12) Priority + Preemption (bonus: multi-tenant + safety)\n",
    "\n",
    "Pod priority affects scheduling order and can preempt lower-priority pods when there isn’t capacity; this is powerful but risky and can violate assumptions like quorum. \n",
    "**Interview soundbite:** “Priority is an ops tool; misuse can cascade failures in shared clusters.”\n",
    "\n",
    "---\n",
    "\n",
    "### If you only memorize **one “smart insight”** for tomorrow (your HPA+VPA point)\n",
    "\n",
    "> “HPA on CPU uses CPU% relative to requests, not limits. If VPA changes those requests while HPA is scaling on them, they can fight and ‘double scale’ because they’re not aware of each other.”  \n",
    "\n",
    "If you want, paste **one interview scenario** you expect (e.g., “latency spike + HPA not scaling” or “rollout caused outage”), and I’ll map it to the exact patterns above in a tight answer outline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90849d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "814a12b2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
