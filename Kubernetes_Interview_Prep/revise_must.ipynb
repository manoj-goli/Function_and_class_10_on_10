{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e02dbf8",
   "metadata": {},
   "source": [
    "Here's a focused cheatsheet for your Kubernetes interview preparation. I've organized it by category with the **must-know** topics for each.\n",
    "\n",
    "---\n",
    "\n",
    "# Kubernetes Interview Cheatsheet\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Auto Scaling\n",
    "\n",
    "### Core Concepts\n",
    "| Component | What it does | Key Interview Points |\n",
    "|-----------|--------------|----------------------|\n",
    "| **HPA (Horizontal Pod Autoscaler)** | Scales pods in/out based on metrics | • Works on **requests**, not limits<br>• Metrics server must be installed<br>• Supports CPU, memory, custom metrics |\n",
    "| **VPA (Vertical Pod Autoscaler)** | Adjusts CPU/memory requests of existing pods | • Can cause pod restarts<br>• Three modes: Off, Auto, Initial |\n",
    "| **Cluster Autoscaler** | Adds/removes worker nodes | • Works with cloud provider APIs<br>• Respects pod disruption budgets<br>• Won't scale down nodes with non-movable pods |\n",
    "| **Metrics Server** | Provides resource usage data | • In-memory, no persistent storage<br>• Required for HPA to work |\n",
    "\n",
    "### Key Formulas\n",
    "```\n",
    "Desired Replicas = ceil(current_replicas × (current_value / desired_value))\n",
    "\n",
    "Example: CPU at 80%, target 50% → 2 × (80/50) = 3.2 → 4 replicas\n",
    "```\n",
    "\n",
    "### Must-Know\n",
    "- **HPA can't scale to zero** (use KEDA for that)\n",
    "- **Cooldown periods** prevent thrashing\n",
    "- **Custom metrics** require adapter (Prometheus, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Service Discovery\n",
    "\n",
    "### Core Concepts\n",
    "| Component | What it does | Key Interview Points |\n",
    "|-----------|--------------|----------------------|\n",
    "| **Service** | Stable endpoint for pods | • Selector-based pod discovery<br>• Gets a stable ClusterIP and DNS name |\n",
    "| **kube-proxy** | Implements Service on each node | • Modes: iptables (default), IPVS, userspace<br>• Watches API server for Service changes |\n",
    "| **CoreDNS** | Internal DNS server | • Pods resolve services as: `svc-name.namespace.svc.cluster.local`<br>• Automatically configured for all Services |\n",
    "| **Endpoints / EndpointSlices** | Tracks healthy pod IPs for a Service | • Updated automatically when pods change<br>• EndpointSlices scale better for large clusters |\n",
    "\n",
    "### DNS Resolution Patterns\n",
    "```\n",
    "my-service                    → same namespace\n",
    "my-service.default            → specific namespace\n",
    "my-service.default.svc.cluster.local → fully qualified\n",
    "```\n",
    "\n",
    "### Must-Know\n",
    "- **Headless Services** (`clusterIP: None`) → Direct pod DNS records\n",
    "- **Environment variables** (deprecated) → DNS is preferred\n",
    "- **Service without selector** → Manual endpoint management\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Load Balancer\n",
    "\n",
    "### Core Concepts\n",
    "| Component | Type | Use Case |\n",
    "|-----------|------|----------|\n",
    "| **ClusterIP** | Internal | Default, only inside cluster |\n",
    "| **NodePort** | External | Exposes on each node's IP:30000-32767 |\n",
    "| **LoadBalancer** | External | Cloud provider LB integration |\n",
    "| **Ingress** | External L7 | HTTP/HTTPS routing, host/path based |\n",
    "\n",
    "### Traffic Flow\n",
    "```\n",
    "External → LoadBalancer (cloud LB) → NodePort → Service → Pods\n",
    "External → Ingress (controller) → Service → Pods\n",
    "```\n",
    "\n",
    "### Must-Know\n",
    "| Concept | Explanation |\n",
    "|---------|-------------|\n",
    "| **ExternalTrafficPolicy** | `Local` preserves client IP, `Cluster` spreads load |\n",
    "| **Session Affinity** | Sticky sessions via `service.spec.sessionAffinity` |\n",
    "| **Ingress Controller** | Not deployed by default (NGINX, Traefik, AWS ALB, etc.) |\n",
    "| **Service Type Order** | LoadBalancer > NodePort > ClusterIP |\n",
    "\n",
    "### Port Ranges\n",
    "- **NodePort:** 30000-32767\n",
    "- **Ingress:** Usually 80/443\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Self Healing\n",
    "\n",
    "### Core Concepts\n",
    "| Component | Heals what? | How? |\n",
    "|-----------|-------------|------|\n",
    "| **kubelet** | Pods on its node | Restarts containers if they crash |\n",
    "| **ReplicaSet controller** | Pod count | Creates new pods if any are missing |\n",
    "| **Deployment controller** | Desired state | Rolls back or recreates if drifted |\n",
    "| **Node controller** | Nodes | Marks nodes NotReady after timeout |\n",
    "| **Control Plane** | Itself | Static pods or systemd restarts |\n",
    "\n",
    "### Must-Know Checks\n",
    "\n",
    "**Liveness Probe:**\n",
    "```yaml\n",
    "livenessProbe:\n",
    "  httpGet:\n",
    "    path: /health\n",
    "    port: 8080\n",
    "  initialDelaySeconds: 30\n",
    "  periodSeconds: 10\n",
    "```\n",
    "→ If fails, **kubelet restarts the container**\n",
    "\n",
    "**Readiness Probe:**\n",
    "```yaml\n",
    "readinessProbe:\n",
    "  httpGet:\n",
    "    path: /ready\n",
    "    port: 8080\n",
    "```\n",
    "→ If fails, **pod removed from Service endpoints**\n",
    "\n",
    "**Startup Probe:**\n",
    "→ For slow-starting apps, protects liveness during boot\n",
    "\n",
    "### Failure Scenarios\n",
    "| Failure | What heals it? |\n",
    "|---------|----------------|\n",
    "| Pod crashes | kubelet restarts it |\n",
    "| Node dies | Pods reschedule on other nodes (if not DaemonSet) |\n",
    "| Deployment misconfiguration | ReplicaSet creates correct pods |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Zero Downtime Deployments\n",
    "\n",
    "### Core Concepts\n",
    "| Strategy | How it works | Downtime? |\n",
    "|----------|--------------|-----------|\n",
    "| **RollingUpdate** | Replaces pods one by one | None if done right |\n",
    "| **Blue/Green** | New version fully deployed, then switch | None |\n",
    "| **Canary** | Small % of traffic to new version | None |\n",
    "| **Recreate** | Kills all old, creates new | Yes |\n",
    "\n",
    "### RollingUpdate Configuration\n",
    "```yaml\n",
    "strategy:\n",
    "  type: RollingUpdate\n",
    "  rollingUpdate:\n",
    "    maxSurge: 25%        # Extra pods during update\n",
    "    maxUnavailable: 25%   # Pods that can be down\n",
    "```\n",
    "\n",
    "### Must-Have for Zero Downtime\n",
    "\n",
    "| Requirement | Why |\n",
    "|-------------|-----|\n",
    "| **Readiness Probes** | Don't send traffic to starting pods |\n",
    "| **PodDisruptionBudget** | Prevent voluntary evictions from taking all pods |\n",
    "| **Graceful Shutdown** | App handles SIGTERM, finishes requests |\n",
    "| **Multiple replicas** | At least 2-3 to survive pod loss |\n",
    "| **Anti-affinity** | Spread pods across nodes |\n",
    "\n",
    "### PodDisruptionBudget Example\n",
    "```yaml\n",
    "apiVersion: policy/v1\n",
    "kind: PodDisruptionBudget\n",
    "metadata:\n",
    "  name: app-pdb\n",
    "spec:\n",
    "  minAvailable: 2        # Or maxUnavailable: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: my-app\n",
    "```\n",
    "\n",
    "### Common Pitfalls\n",
    "| Mistake | Consequence |\n",
    "|---------|-------------|\n",
    "| No readiness probe | Traffic sent to starting/broken pods |\n",
    "| `terminationGracePeriodSeconds` too short | Requests cut mid-process |\n",
    "| Database schema changes incompatible | New pods fail against old DB |\n",
    "| No PodDisruptionBudget | Node drain kills all pods at once |\n",
    "\n",
    "---\n",
    "\n",
    "## Bonus: Quick Comparison Table\n",
    "\n",
    "| Category | Key Components | Most Important Concept |\n",
    "|----------|----------------|------------------------|\n",
    "| **Auto Scaling** | HPA, VPA, Cluster Autoscaler | HPA scales based on **requests**, not actual usage |\n",
    "| **Service Discovery** | Service, CoreDNS, Endpoints | DNS: `svc.namespace.svc.cluster.local` |\n",
    "| **Load Balancer** | Service (NodePort/LB), Ingress | Ingress is L7, Service is L4 |\n",
    "| **Self Healing** | kubelet, controllers, probes | Liveness vs Readiness probes |\n",
    "| **Zero Downtime** | RollingUpdate, PDB, probes | maxSurge + maxUnavailable = safe updates |\n",
    "\n",
    "---\n",
    "\n",
    "## Final Interview Tips\n",
    "\n",
    "1. **Always mention \"requests vs limits\"** when discussing scheduling/scaling\n",
    "2. **Know the difference:** Liveness (restart) vs Readiness (traffic)\n",
    "3. **For zero downtime:** Always mention readiness probes + PDB\n",
    "4. **For service discovery:** Mention that DNS is the modern way (not env vars)\n",
    "5. **For load balancers:** Know that Ingress requires a controller\n",
    "\n",
    "Good luck with your interview!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d1d43",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
